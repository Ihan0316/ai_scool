{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T08:28:12.894717Z",
     "start_time": "2025-07-22T08:27:43.857019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# ---------- 데이터 로드 ----------\n",
    "train = pd.read_csv('./data/1.titanic_train.csv')\n",
    "test = pd.read_csv('./data/2.titanic_test.csv')\n",
    "submission = pd.read_csv('./data/3.titanic_submission.csv')\n",
    "test_passenger_id = test['PassengerId']\n",
    "\n",
    "# ---------- [Feature Engineering/데이터 가공] ----------\n",
    "for df in [train, test]:\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['FamilyType'] = pd.cut(df['FamilySize'], bins=[0, 1, 4, 20], labels=['Alone', 'Small', 'Large'])\n",
    "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "def extract_title(name):\n",
    "    match = re.search(r',\\s*([^.]*)\\.', name)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return 'Unknown'\n",
    "for df in [train, test]:\n",
    "    df['Title'] = df['Name'].apply(extract_title)\n",
    "    df['Title'] = df['Title'].replace(\n",
    "        ['Lady', 'Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'], 'Rare'\n",
    "    )\n",
    "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "for df in [train, test]:\n",
    "    df['Age*Pclass'] = df['Age'] * df['Pclass']\n",
    "    df['Sex*Pclass'] = df.apply(lambda row: 0 if row['Sex']=='male' else 1, axis=1) * df['Pclass']\n",
    "for df in [train, test]:\n",
    "    df['FarePerPerson'] = df['Fare'] / (df['FamilySize'])\n",
    "    df['Fare_log'] = np.log1p(df['Fare'])\n",
    "for df in [train, test]:\n",
    "    df['Deck'] = df['Cabin'].fillna('U').map(lambda x: x[0])\n",
    "for df in [train, test]:\n",
    "    for title in df['Title'].unique():\n",
    "        med = df.loc[df['Title']==title, 'Age'].median()\n",
    "        df.loc[(df['Title']==title) & (df['Age'].isnull()), 'Age'] = med\n",
    "for df in [train, test]:\n",
    "    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "for df in [test]:\n",
    "    for pclass in df['Pclass'].unique():\n",
    "        med = df.loc[df['Pclass']==pclass, 'Fare'].median()\n",
    "        df.loc[(df['Pclass']==pclass) & (df['Fare'].isnull()), 'Fare'] = med\n",
    "\n",
    "drop_cols = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "train = train.drop(drop_cols, axis=1)\n",
    "test = test.drop(drop_cols, axis=1)\n",
    "\n",
    "numerical_features = train.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'Survived' in numerical_features: numerical_features.remove('Survived')\n",
    "categorical_features = train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "X = train.drop('Survived', axis=1)\n",
    "y = train['Survived']\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ----------- XGBoost 최적화 -----------\n",
    "xgb_base = XGBClassifier(random_state=42, eval_metric='logloss', tree_method='hist')\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'max_depth': randint(2, 10),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'gamma': uniform(0, 2),\n",
    "    'reg_alpha': uniform(0, 2),\n",
    "    'reg_lambda': uniform(0, 2)\n",
    "}\n",
    "rand_search_xgb = RandomizedSearchCV(\n",
    "    xgb_base, param_distributions=param_dist_xgb, n_iter=10, cv=kfold, scoring='accuracy',\n",
    "    verbose=0, n_jobs=-1, random_state=42\n",
    ")\n",
    "rand_search_xgb.fit(X_train, y_train)\n",
    "best_xgb_params = rand_search_xgb.best_params_\n",
    "\n",
    "# ----------- LightGBM 최적화 -----------\n",
    "lgbm_base = LGBMClassifier(random_state=42, verbosity=-1)\n",
    "param_dist_lgbm = {\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'max_depth': randint(2, 10),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'reg_alpha': uniform(0, 2),\n",
    "    'reg_lambda': uniform(0, 2)\n",
    "}\n",
    "rand_search_lgbm = RandomizedSearchCV(\n",
    "    lgbm_base, param_distributions=param_dist_lgbm, n_iter=10, cv=kfold, scoring='accuracy',\n",
    "    verbose=0, n_jobs=-1, random_state=42\n",
    ")\n",
    "rand_search_lgbm.fit(X_train, y_train)\n",
    "best_lgbm_params = rand_search_lgbm.best_params_\n",
    "best_lgbm_params['verbosity'] = -1\n",
    "\n",
    "# ----------- RandomForest -----------\n",
    "rfc_base = RandomForestClassifier(random_state=42)\n",
    "param_grid_rfc = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 7, 12, None],\n",
    "    'min_samples_split': [2, 4, 8]\n",
    "}\n",
    "grid_search_rfc = GridSearchCV(\n",
    "    rfc_base, param_grid_rfc, cv=kfold, scoring='accuracy',\n",
    "    verbose=0, n_jobs=-1\n",
    ")\n",
    "grid_search_rfc.fit(X_train, y_train)\n",
    "best_rfc_params = grid_search_rfc.best_params_\n",
    "\n",
    "# ----------- CatBoost -----------\n",
    "catboost_base = CatBoostClassifier(random_state=42, verbose=0)\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(60, 200),\n",
    "    'depth': randint(3, 9),\n",
    "    'learning_rate': uniform(0.03, 0.1),\n",
    "    'l2_leaf_reg': uniform(0.5, 5)\n",
    "}\n",
    "rand_search_cat = RandomizedSearchCV(\n",
    "    catboost_base, param_distributions=param_dist_cat, n_iter=8, cv=kfold, scoring='accuracy',\n",
    "    verbose=0, n_jobs=-1, random_state=42\n",
    ")\n",
    "rand_search_cat.fit(X_train, y_train)\n",
    "best_cat_params = rand_search_cat.best_params_\n",
    "best_cat_params['verbose'] = 0\n",
    "\n",
    "# ----------- 모델 객체 생성 (최적 파라미터) -----------\n",
    "xgb_final = XGBClassifier(**best_xgb_params, random_state=42, eval_metric='logloss', tree_method='hist')\n",
    "lgbm_final = LGBMClassifier(**best_lgbm_params, random_state=42)\n",
    "rfc_final = RandomForestClassifier(**best_rfc_params, random_state=42)\n",
    "cat_final = CatBoostClassifier(**best_cat_params, random_state=42)\n",
    "\n",
    "# ----------- 최적 점수 및 파라미터 표시 -----------\n",
    "print(\"=== 하이퍼파라미터 최적화 결과 ===\")\n",
    "model_results = {\n",
    "    \"XGBoost\": (rand_search_xgb.best_score_, rand_search_xgb.best_params_),\n",
    "    \"LightGBM\": (rand_search_lgbm.best_score_, rand_search_lgbm.best_params_),\n",
    "    \"RandomForest\": (grid_search_rfc.best_score_, grid_search_rfc.best_params_),\n",
    "    \"CatBoost\": (rand_search_cat.best_score_, rand_search_cat.best_params_)\n",
    "}\n",
    "for name, (score, params) in model_results.items():\n",
    "    print(f\"{name:12} | score={score:.4f} | params={params}\")\n",
    "\n",
    "# 어떤 모델이 최고 성능인지 출력\n",
    "best_model = max(model_results.items(), key=lambda x: x[1][0])\n",
    "print(f\"\\n>>> 가장 성능이 좋았던 모델: {best_model[0]} (score={best_model[1][0]:.4f}) \")\n",
    "print(f\"    최적 파라미터: {best_model[1][1]}\\n\")\n",
    "\n",
    "# ----------- 스태킹 앙상블 -----------\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_final),\n",
    "        ('lgbm', lgbm_final),\n",
    "        ('rf', rfc_final),\n",
    "        ('cat', cat_final)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(random_state=42, max_iter=1000),\n",
    "    cv=kfold,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "# ----------- 학습 데이터 평가 -----------\n",
    "y_pred_train = stacking.predict(X_train)\n",
    "y_pred_train_proba = stacking.predict_proba(X_train)\n",
    "print(\"\\n=== [Stacking 앙상블: 학습 데이터] ===\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(\"R2  :\", r2_score(y_train, y_pred_train))\n",
    "print(\"MAE :\", mean_absolute_error(y_train, y_pred_train))\n",
    "print(\"MSE :\", mean_squared_error(y_train, y_pred_train))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "print(\"Log Loss:\", log_loss(y_train, y_pred_train_proba))\n",
    "\n",
    "# ----------- 검증 데이터 평가 -----------\n",
    "y_pred_val_proba = stacking.predict_proba(X_val)\n",
    "y_pred_val = stacking.predict(X_val)\n",
    "print(\"\\n=== [Stacking 앙상블: 검증 데이터] ===\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(\"R2  :\", r2_score(y_val, y_pred_val))\n",
    "print(\"MAE :\", mean_absolute_error(y_val, y_pred_val))\n",
    "print(\"MSE :\", mean_squared_error(y_val, y_pred_val))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_val, y_pred_val)))\n",
    "print(\"Log Loss:\", log_loss(y_val, y_pred_val_proba))\n",
    "\n",
    "# ----------- 전체 데이터로 재학습 -----------\n",
    "stacking.fit(X_processed, y)\n",
    "\n",
    "# ----------- 최종 제출 예측 -----------\n",
    "y_test_pred = stacking.predict(X_test_processed)\n",
    "submission['Survived'] = y_test_pred\n",
    "submission['PassengerId'] = test_passenger_id\n",
    "submission.to_csv('titanic_stacking_submission.csv', index=False)\n",
    "print(\"\\n제출 파일 저장 완료!(titanic_stacking_submission.csv)\")\n"
   ],
   "id": "fd283fc8e69333b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 하이퍼파라미터 최적화 결과 ===\n",
      "XGBoost      | score=0.8331 | params={'colsample_bytree': 0.5924272277627636, 'gamma': 1.9391692555291171, 'learning_rate': 0.08751328233611146, 'max_depth': 3, 'n_estimators': 251, 'reg_alpha': 1.7896547008552977, 'reg_lambda': 1.1957999576221703, 'subsample': 0.9609371175115584}\n",
      "LightGBM     | score=0.8231 | params={'colsample_bytree': 0.7838501639099957, 'learning_rate': 0.01313132924555586, 'max_depth': 3, 'n_estimators': 267, 'reg_alpha': 0.8995082667395313, 'reg_lambda': 0.7903004720036289, 'subsample': 0.9633294328968971, 'verbosity': -1}\n",
      "RandomForest | score=0.8331 | params={'max_depth': 12, 'min_samples_split': 8, 'n_estimators': 100}\n",
      "CatBoost     | score=0.8264 | params={'depth': 5, 'iterations': 147, 'l2_leaf_reg': 2.168543055695109, 'learning_rate': 0.044286681792194076, 'verbose': 0}\n",
      "\n",
      ">>> ⭐ 가장 성능이 좋았던 모델: XGBoost (score=0.8331) \n",
      "    최적 파라미터: {'colsample_bytree': 0.5924272277627636, 'gamma': 1.9391692555291171, 'learning_rate': 0.08751328233611146, 'max_depth': 3, 'n_estimators': 251, 'reg_alpha': 1.7896547008552977, 'reg_lambda': 1.1957999576221703, 'subsample': 0.9609371175115584}\n",
      "\n",
      "\n",
      "=== [Stacking 앙상블: 학습 데이터] ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91       373\n",
      "           1       0.86      0.82      0.84       232\n",
      "\n",
      "    accuracy                           0.88       605\n",
      "   macro avg       0.88      0.87      0.87       605\n",
      "weighted avg       0.88      0.88      0.88       605\n",
      "\n",
      "R2  : 0.5036169917722104\n",
      "MAE : 0.11735537190082644\n",
      "MSE : 0.11735537190082644\n",
      "RMSE: 0.34257170329848674\n",
      "Log Loss: 0.30622658016928805\n",
      "\n",
      "=== [Stacking 앙상블: 검증 데이터] ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90        94\n",
      "           1       0.90      0.74      0.81        58\n",
      "\n",
      "    accuracy                           0.87       152\n",
      "   macro avg       0.88      0.84      0.86       152\n",
      "weighted avg       0.87      0.87      0.87       152\n",
      "\n",
      "R2  : 0.44240645634629505\n",
      "MAE : 0.13157894736842105\n",
      "MSE : 0.13157894736842105\n",
      "RMSE: 0.3627381250550058\n",
      "Log Loss: 0.3427903574501149\n",
      "\n",
      "제출 파일 저장 완료!(titanic_stacking_submission.csv)\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
